{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff64401",
   "metadata": {},
   "source": [
    "## Ragas Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff21278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from ragas.dataset import Dataset\n",
    "import ast\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4edfdffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAGAS imports\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,AnswerRelevancy,\n",
    "    ContextPrecision,ContextRecall,\n",
    "    AnswerCorrectness,AnswerSimilarity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import your existing components\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from weaviate_client import init_weaviate, WeaviateHybridRetriever\n",
    "from rag_utils import run_agent_query, create_rag_agent\n",
    "from config import GROQ_API_KEY, COLLECTION_EMBED_MAP\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c855786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e884c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 16:22:50.521 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.294 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-22 16:22:51.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.803 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.803 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:51.803 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:53.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:53.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-22 16:22:53.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\KarthikKodam(Quadran\\AppData\\Local\\Temp\\ipykernel_19348\\4144273904.py:22: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  wrapped_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\embeddings\\base.py:46: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  return self.new_target(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "weaviate_client = init_weaviate()\n",
    "\n",
    "# Initialize Groq LLM for evaluation\n",
    "groq_llm = ChatGroq(\n",
    "    # model=\"openai/gpt-oss-120b\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    n = 1\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Wrap for RAGAS\n",
    "wrapped_llm = LangchainLLMWrapper(groq_llm)\n",
    "wrapped_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a540d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation Queries and Ground Truth\n",
    "evaluation_data = [\n",
    "    {\n",
    "        \"question\": \"\"\"What was the Supreme Court of Canada's decision in the case concerning Fisheries Jurisdiction between Spain and Canada?\"\"\",\n",
    "        \"ground_truth\": \"\"\"The Supreme Court of Canada did not make a decision in the case concerning Fisheries Jurisdiction between Spain and Canada, as the case was heard by the International Court of Justice. The ICJ ruled that it had no jurisdiction to adjudicateon the dispute brought by Spain in 1995.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"Why did Judge Koroma dissent from the Court's finding that it lacked jurisdiction to respond to the WHO's request? from Legality of the Use by a State of Nuclear Weapons in Armed Conflict case\"\"\",\n",
    "        \"ground_truth\": \"\"\"Judge Koroma dissented because he believed the Court misconstrued the question. In his view, the question related to the health and environmental effects of nuclear weapons, which he maintained fell \"eminently within the competence and scope of the agency's activities.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"What was Judge Weeramantry's view on the Court's application of the \"principle of speciality\" to the WHO? from Legality of the Use by a State of Nuclear Weapons in Armed Conflict case \"\"\",\n",
    "        \"ground_truth\": \"\"\"Judge Weeramantry disagreed with the Court's rigid application of the \"principle of speciality,\" which took the question of legality out of the WHO's area of concern just because peace and security were within the concerns of the Security Council.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"What did the Federal Court of Appeal hold regarding the proper constitution of the Charter challenge? from Canadian Council for Refugees v. Canada (Citizenship and Immigration) case\"\"\",\n",
    "        \"ground_truth\": \"\"\"The Federal Court of Appeal held that the Charter challenge was not properly constituted because it should have been directed at other forms of state action, specifically the administrative reviews required by s. 102(3) of the IRPA, rather than s. 159.3 of the IRPR.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"Does the evidence support the Federal Court judge's finding that detention of returnees in the United States is \"automatic\"? from case Canadian Council for Refugees v. Canada (Citizenship and Immigration)\"\"\",\n",
    "        \"ground_truth\": \"\"\"The evidence does not support the finding that detention is \"automatic.\" The text states that detention is not universally applied and that returnees' risks of detention vary on a case-by-case basis.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a3daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name law-ai/InLegalBERT. Creating a new one with mean pooling.\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\Agentic_RAG5\\rag_utils.py:84: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\Agentic_RAG5\\rag_utils.py:102: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG System\n",
    "\n",
    "# Create retriever\n",
    "collection_name = \"InLegalBERT_Chunks\"\n",
    "retriever = WeaviateHybridRetriever(\n",
    "    client=weaviate_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_model_name=COLLECTION_EMBED_MAP[collection_name][\"model\"],\n",
    "    alpha=0.5,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# Create RAG agent\n",
    "agent = create_rag_agent(\n",
    "    retriever=retriever,\n",
    "    # selected_model=\"openai/gpt-oss-120b\",\n",
    "    selected_model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    alpha=0.5,\n",
    "    top_k=3,\n",
    "    show_metadata=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Answers and Retrieve Contexts\n",
    "\n",
    "for i, data in enumerate(evaluation_data, 1):\n",
    "    print(f\"\\nProcessing query {i}/{len(evaluation_data)}\")\n",
    "    print(f\"Query: {data['question'][:80]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Run RAG system\n",
    "        result = run_agent_query(\n",
    "            agent=agent,\n",
    "            query=data[\"question\"],\n",
    "            retriever=retriever,\n",
    "            show_metadata=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        data[\"answer\"] = result[\"generated_answer\"]\n",
    "        data[\"contexts\"] = result[\"contexts\"]\n",
    "        \n",
    "        print(f\"  ‚úÖ Contexts retrieved ({len(data['contexts'])} docs)\")\n",
    "        print(f\"  Answer preview: {data['answer'][:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        data[\"answer\"] = f\"Error: {str(e)}\"\n",
    "        data[\"contexts\"] = []\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4efb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RAGAS dataset created with 3 samples\n"
     ]
    }
   ],
   "source": [
    "# Create dataset for RAGAS\n",
    "ragas_dataset = Dataset.from_list([\n",
    "    {\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"contexts\": data[\"contexts\"],\n",
    "        \"ground_truth\": data[\"ground_truth\"]\n",
    "    }\n",
    "    for data in evaluation_data\n",
    "])\n",
    "\n",
    "print(f\"üìä RAGAS dataset created with {len(ragas_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45adc16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c483290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample data:\n",
      "\n",
      "Sample 1:\n",
      "Question: What was the Supreme Court of Canada's decision in the case ...\n",
      "Answer: The Supreme Court of Canada did not hear the case concerning Fisheries Jurisdict...\n",
      "Contexts: 3 retrieved\n",
      "\n",
      "Sample 2:\n",
      "Question: Why did Judge Koroma dissent from the Court's finding that i...\n",
      "Answer: Judge Koroma dissented from the Court's finding that it lacked jurisdiction to r...\n",
      "Contexts: 3 retrieved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Sample data:\")\n",
    "for i in range(min(2, len(ragas_dataset))):\n",
    "    sample = ragas_dataset[i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Question: {sample['question'][:60]}...\")\n",
    "    print(f\"Answer: {sample['answer'][:80]}...\")\n",
    "    print(f\"Contexts: {len(sample['contexts'])} retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738db845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAGAS Metrics\n",
    "metrics = [\n",
    "    Faithfulness(llm=wrapped_llm),\n",
    "    AnswerRelevancy(llm=wrapped_llm, embeddings=wrapped_embeddings),\n",
    "    ContextPrecision(llm=wrapped_llm),\n",
    "    ContextRecall(llm=wrapped_llm),\n",
    "    AnswerCorrectness(llm=wrapped_llm, embeddings=wrapped_embeddings),\n",
    "    AnswerSimilarity(embeddings=wrapped_embeddings)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcaf61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ RAGAS metrics initialized:\n",
      "  - faithfulness\n",
      "  - answer_relevancy\n",
      "  - context_precision\n",
      "  - context_recall\n",
      "  - answer_correctness\n",
      "  - answer_similarity\n"
     ]
    }
   ],
   "source": [
    "print(\"üßÆ RAGAS metrics initialized:\")\n",
    "for metric in metrics:\n",
    "    print(f\"  - {metric.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a60c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     results = evaluate(\n",
    "#         dataset=ragas_dataset,\n",
    "#         metrics=metrics\n",
    "#     )\n",
    "#     print(\"‚úÖ Evaluation completed!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Evaluation failed: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b4b2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'results' in locals():    \n",
    "#     df = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f4f8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1049b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"eval_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "450576d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the Supreme Court of Canada's decisio...</td>\n",
       "      <td>['lack of title to act on the high seas agains...</td>\n",
       "      <td>The Supreme Court of Canada did not hear the c...</td>\n",
       "      <td>The Supreme Court of Canada did not make a dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did Judge Koroma dissent from the Court's ...</td>\n",
       "      <td>['principles of treaty interpretation and shou...</td>\n",
       "      <td>Judge Koroma dissented from the Court's findin...</td>\n",
       "      <td>Judge Koroma dissented because he believed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was Judge Weeramantry's view on the Court...</td>\n",
       "      <td>['principles of treaty interpretation and shou...</td>\n",
       "      <td>Judge Weeramantry disagreed with the Court's r...</td>\n",
       "      <td>Judge Weeramantry disagreed with the Court's r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What was the Supreme Court of Canada's decisio...   \n",
       "1  Why did Judge Koroma dissent from the Court's ...   \n",
       "2  What was Judge Weeramantry's view on the Court...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  ['lack of title to act on the high seas agains...   \n",
       "1  ['principles of treaty interpretation and shou...   \n",
       "2  ['principles of treaty interpretation and shou...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The Supreme Court of Canada did not hear the c...   \n",
       "1  Judge Koroma dissented from the Court's findin...   \n",
       "2  Judge Weeramantry disagreed with the Court's r...   \n",
       "\n",
       "                                           reference  \n",
       "0  The Supreme Court of Canada did not make a dec...  \n",
       "1  Judge Koroma dissented because he believed the...  \n",
       "2  Judge Weeramantry disagreed with the Court's r...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"eval_dataset.csv\") \n",
    "eval_df = df2[['user_input','retrieved_contexts','response','reference']].head()\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d282c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocess retrieved_contexts ---\n",
    "eval_df[\"retrieved_contexts\"] = eval_df[\"retrieved_contexts\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\_analytics.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\KarthikKodam(Quadran\\\\AppData\\\\Local\\\\ragas\\\\ragas\\\\uuid.json' mode='r' encoding='cp1252'>\n",
      "  user_id = json.load(open(uuid_filepath))[\"userid\"]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\base.py:516: DeprecationWarning: The function _single_turn_ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  self._single_turn_ascore(sample=sample, callbacks=group_cm),\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextPrecisionWithReference instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Evaluating:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:32<00:05,  5.82s/it]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_answer_correctness.py:260: DeprecationWarning: The function ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  similarity_score = await self.answer_similarity.ascore(\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:39<00:00,  6.60s/it]\n",
      "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\base.py:516: DeprecationWarning: The function _single_turn_ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  self._single_turn_ascore(sample=sample, callbacks=group_cm),\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextPrecisionWithReference instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Evaluating:  17%|‚ñà‚ñã        | 1/6 [00:15<01:19, 15.94s/it]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_answer_correctness.py:260: DeprecationWarning: The function ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  similarity_score = await self.answer_similarity.ascore(\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:57<00:00,  9.65s/it]\n",
      "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\base.py:516: DeprecationWarning: The function _single_turn_ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  self._single_turn_ascore(sample=sample, callbacks=group_cm),\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextPrecisionWithReference instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: DeprecationWarning: The function _ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use LLMContextRecall instead.\n",
      "  return await self._ascore(row, callbacks)\n",
      "Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Evaluating:  17%|‚ñà‚ñã        | 1/6 [00:17<01:26, 17.24s/it]c:\\Users\\KarthikKodam(Quadran\\vs\\Capstone\\genai\\Lib\\site-packages\\ragas\\metrics\\_answer_correctness.py:260: DeprecationWarning: The function ascore was deprecated in 0.2, and will be removed in the 0.3 release. Use single_turn_ascore instead.\n",
      "  similarity_score = await self.answer_similarity.ascore(\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:07<00:00, 11.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ All metrics calculated and saved to ragas_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for i, row in eval_df.iterrows():\n",
    "    # Create a 1-row DataFrame\n",
    "    single_row_df = pd.DataFrame([row])\n",
    "\n",
    "    # Convert pandas ‚Üí HuggingFace Dataset\n",
    "    hf_dataset = HFDataset.from_pandas(single_row_df)\n",
    "\n",
    "    try:\n",
    "        eval_result = evaluate(hf_dataset, metrics=metrics)\n",
    "        eval_df_row = eval_result.to_pandas()\n",
    "        results_list.append(eval_df_row)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error at row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Sleep to respect rate limits\n",
    "    time.sleep(20)\n",
    "\n",
    "final_results = pd.concat(results_list, ignore_index=True)\n",
    "final_results.to_csv(\"ragas_results.csv\", index=False)\n",
    "print(\"üéâ All metrics calculated and saved to ragas_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the Supreme Court of Canada's decisio...</td>\n",
       "      <td>['lack of title to act on the high seas agains...</td>\n",
       "      <td>The Supreme Court of Canada did not hear the c...</td>\n",
       "      <td>The Supreme Court of Canada did not make a dec...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536990</td>\n",
       "      <td>0.947959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did Judge Koroma dissent from the Court's ...</td>\n",
       "      <td>['principles of treaty interpretation and shou...</td>\n",
       "      <td>Judge Koroma dissented from the Court's findin...</td>\n",
       "      <td>Judge Koroma dissented because he believed the...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.352130</td>\n",
       "      <td>0.741853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was Judge Weeramantry's view on the Court...</td>\n",
       "      <td>['principles of treaty interpretation and shou...</td>\n",
       "      <td>Judge Weeramantry disagreed with the Court's r...</td>\n",
       "      <td>Judge Weeramantry disagreed with the Court's r...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.357129</td>\n",
       "      <td>0.761849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What was the Supreme Court of Canada's decisio...   \n",
       "1  Why did Judge Koroma dissent from the Court's ...   \n",
       "2  What was Judge Weeramantry's view on the Court...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  ['lack of title to act on the high seas agains...   \n",
       "1  ['principles of treaty interpretation and shou...   \n",
       "2  ['principles of treaty interpretation and shou...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The Supreme Court of Canada did not hear the c...   \n",
       "1  Judge Koroma dissented from the Court's findin...   \n",
       "2  Judge Weeramantry disagreed with the Court's r...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  The Supreme Court of Canada did not make a dec...      0.571429   \n",
       "1  Judge Koroma dissented because he believed the...      0.833333   \n",
       "2  Judge Weeramantry disagreed with the Court's r...      0.857143   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  answer_correctness  \\\n",
       "0               NaN           0.833333             1.0            0.536990   \n",
       "1               NaN           1.000000             1.0            0.352130   \n",
       "2               NaN           0.833333             1.0            0.357129   \n",
       "\n",
       "   answer_similarity  \n",
       "0           0.947959  \n",
       "1           0.741853  \n",
       "2           0.761849  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"ragas_results.csv\") \n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
